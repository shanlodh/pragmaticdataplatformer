Current AI spring unfolds in an era of mass cloud computing. But rising cloud costs ([cloudflation](https://www.forbes.com/sites/forbestechcouncil/2023/02/23/three-tips-to-navigate-cloud-flation/)) means enterprises must be extremely deliberate how they leverage public cloud and demonstrate clear benefits from doing so. This note describes specialised AI as an area where public cloud offers comparative benefits towards faster GTM with AI 

**"It's all about chips in the AI war"** (image below, [article link](https://www.ft.com/content/0a16c45f-5739-43ad-abdc-1b91afa83e0d)) as chips drive AI model training and inference on rising volumes of (unstructured) data and ever larger [foundation models](https://en.wikipedia.org/wiki/Foundation_models). But the learning curve and maintenance overheads to deploy optimal chips-per-compute (specially GPUs) are steep. Utilising public cloud for such deployments are thus a key driver in AI-led innovation. Data platforms can help companies succeed with AI by providing such cloud native optimised compute environments with IaC scalability, PEP security and programmatic cost controls  

In short, to win with AI, **data platformers know thy chips**: 

- **Tabular data - Boosting Algorithms - Intel AI Kit API::** while foundation models are current cynosures, research suggests tree-based models still outperform deep learning on tabular data ([arXiv paper](https://arxiv.org/abs/2207.08815)), particularly error-correction or boosting algorithms [XGBoost / CatBoost / LightGBM / AdaBoost](https://www.geeksforgeeks.org/gradientboosting-vs-adaboost-vs-xgboost-vs-catboost-vs-lightgbm/). A cost controlled way to speed up ML workloads for tabular data significantly is with the [Intel AI Kit API](https://www.intel.com/content/www/us/en/developer/tools/oneapi/ai-analytics-toolkit.html#gs.qdpxi0) pip installed for [the Scikit-learn extension, for example](https://pypi.org/project/scikit-learn-intelex/) (includes XGBoost, LightGBM). Other API extensions exist for [PyTorch](https://pypi.org/project/intel-extension-for-pytorch/), [TensorFlow](https://pypi.org/project/intel-extension-for-tensorflow/) and [Transformers](https://pypi.org/project/intel-extension-for-transformers/). Even before GPU offloading (see below) this approach provides [10X++ training, inference pick-ups on several common Scikit learn estimators](https://github.com/intel/scikit-learn-intelex#-acceleration) 

- **Intel AI Kit API - AWS, Azure - Databricks:** using public cloud (AWS, Azure) and IaC we quickly standup VMs with specific compute optimised Intel processors suitable for the Intel AI Kit API and benefit from advanced vector instructions, faster BLAS and scalable multithreading that [drives the API's performance](https://www.intel.com/content/www/us/en/developer/tools/oneapi/onedal.html#gs.r3jyga). On AWS, for e.g., it can be a C6i EC2 or on Azure the Standard_D32s_v3 VM (check your cloud region for latest). Further, using the VMs in compute clusters like Databricks leverages other ecosystem features such as [Delta Live Tables](https://www.databricks.com/product/delta-live-tables), [Auto Loader](https://learn.microsoft.com/en-us/azure/databricks/ingestion/auto-loader/)and [Unity Catalog](https://www.databricks.com/product/unity-catalog)  

- **Unstructured data - Foundation Models - GPUs:** [latest MLPerf(TM) results of Nov, 2022](https://mlcommons.org/en/training-normal-21/) shows universal use of GPUs for top results in training unstructured image, object, speech and NLP data. Most relevant reason for faster GPUs here is that GPU's are better at parallel execution of tasks such as sorting (for tree splits) and matrix multiplication (for forward, backward propagations). This was even called out in the [original Transformer paper](https://arxiv.org/abs/1706.03762), *"The Transformer allows for **significantly more parallelization** and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs"* So if you are managing a data platform **deploy GPUs only with right models and systems design**, otherwise performance upticks comes at very high costs 

- **GPUs - CUDA - Driver, Toolkit, SDK:** [CUDA](https://en.wikipedia.org/wiki/CUDA) is the standard API for the market leader, NVIDIA, GPUs. Installing CUDA across [Linux](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html) and [WSL2](https://docs.nvidia.com/cuda/wsl-user-guide/index.html#getting-started-with-cuda-on-wsl) include installing the OS specific driver, toolkit (compiler, runtime library) and language specific SDK. CUDA provides native support for C, C++, Fortran and [Cuda Python](https://developer.nvidia.com/how-to-cuda-python) patches Python to CUDA via the [numba](https://numba.pydata.org/) JIT complier. As links show, operating CUDA is non-trivial and using the public cloud to use pre-built images such as [AWS Deep Learning Machine Images](https://aws.amazon.com/machine-learning/amis/) or [Azure Data Science Virtual Machines](https://azure.microsoft.com/en-us/products/virtual-machines/data-science-virtual-machines/) is a more effective way of using GPU's for accelerating time to market for most organisations. For non-NVIDIA GPUs, consult documentation (e.g., [TPU distribution strategies](https://www.tensorflow.org/guide/tpu#distribution_strategies) for TensorFlow workloads)

- **Application Specific Integrated Circuits (ASIC) - Google TPU, AWS Trainium:** in above MLPerf(TM) results, while NVIDIA is ubiquitous across AI segments and at/near top leader board for most workloads there are some (e.g., [ResNet](https://en.wikipedia.org/wiki/Residual_neural_network), [RetinaNet](https://paperswithcode.com/method/retinanet)) where an ASIC like TPU have shown better results in specific tests. So on GCP you could consider a pre-built machine image with an [ASIC](https://en.wikipedia.org/wiki/Application-specific_integrated_circuit) like [TPU](https://cloud.google.com/tpu/docs/tpus) or [Trainium on AWS](https://aws.amazon.com/machine-learning/trainium/) if these are more suitable for your workloads and/or more cost-effective 

- **STOP PRESS: NVIDIA DGX Cloud-AI-As-A-Service:** [NVIDIA announced on 22 Feb 2023 earnings call](https://www.crn.com/news/components-peripherals/nvidia-teases-dgx-cloud-ai-as-a-service-as-earnings-wow-wall-street) upcoming DGX Cloud offering Cloud-AI-As-A-Service. Already on Oracle Cloud, DGX details for Azure, GCP et al offerings are expected at [March NVIDIA GTC Developer Conference](https://www.nvidia.com/gtc/). Once again, this illustrates the **growing trend of AI PaaS solutions**  

In summary, unless this it's your enterprise's core business area, **leverage data platform team to provide secure, cost-effective cloud based AI PaaS solutions to develop a winning AI strategy** 

<p align="center">
  <img src="https://github.com/shanlodh/pragmaticdataplatformer/blob/main/004_KnowThyChips/Images/ChipsInAI.jpg" />
</p>