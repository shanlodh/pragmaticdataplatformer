Whilst everyone's looking at ChatGPT I also want to go beyond it and try and understand the broader landscape that is evolving rapidly from this remarkable innovation

Following views strictly personal, references through link at bottom: 

- **Is Anyscale the new Databricks:** ["... … OpenAI is very secretive"](https://www.businessinsider.com/openai-chatgpt-trained-on-anyscale-ray-generative-lifelike-ai-models-2022-12?r=US&IR=T), says Ion Stoica, co-founder [Databricks](https://www.databricks.com/), [Anyscale](https://www.anyscale.com/) and [CS Professor at UC, Berkeley]((https://people.eecs.berkeley.edu/~istoica/)). One technology OpenAI does acknowledges using however is from Anyscale (see image below) viz. [Python package Ray](https://pypi.org/project/ray/). Think of Ray being for GPU's what Spark was for CPU's, hence the bullet title. The other innovation from Ray is the [Actor pattern](https://docs.ray.io/en/latest/ray-core/actors.html) which is a stateful worker for distributed processing. Certainly a company to watch, and couple podcasts ([March, 2022](https://www.pythonpodcast.com/anyscale-machine-learning-applications-episode-355) and [April, 2020](https://www.pythonpodcast.com/ray-distributed-computing-episode-258)) with [Anyscale co-founder Robert Nishihara](http://www.robertnishihara.com/) provide more in-depth references to Ray 

- **Get your chips:** LLMs, diffusion, RL all draw heavily on GPU's and [Nvidia is seen as one of the main beneficiaries](https://www.bloomberg.com/news/articles/2023-01-23/nvidia-nvda-is-wall-street-s-top-stock-pick-for-chatgpt-mania) in this space. With reports of ['Code Red' at Google from ChatGPT](https://www.cnet.com/tech/services-and-software/chatgpt-caused-code-red-at-google-report-says/) also wonder if GOOG wants to start making the [TPU](https://cloud.google.com/tpu/docs/tpus) more multi-cloud GA and less GCP focused, specially on the back of the [CHIPS Act](https://www.nist.gov/semiconductors/chips-act)

- **From silent era to talkies:** If you have watched [Babylon](https://www.imdb.com/title/tt10640346/) recently, the current AI landscape, despite the hype, feels like silent-era Hollywood. We have images ([DALL∙E](https://openai.com/dall-e-2/)) and text [[ChatGPT](https://openai.com/blog/chatgpt/)] but these are not fully synchronised. Instead they follow a naive leader-follower pattern (input text, create image or run image, create sub-titles). The 'talkie' era of AI should see multiple generative models generating tokens simultaneously and in synchronised fashion (a la lip-sync in motion pictures). This requires a fundamental new type of cross-entropy going beyond the current [Shannon/Kullback-Leibler](https://en.wikipedia.org/wiki/Entropy_(information_theory)) paradigm that models different distributions over the same random variable. We will need loss functions to cross-evaluate instantaneously tokens across text space and image space for example (if you are aware of developments in this space do mention in comments)

- **Software eats world, AI eats software:** "Software is eating the world" is so BC (before ChatGPT), surely AI is now eating software? Though ChatGPT serves (reasonably) well-formed code in various languages, the underlying engine where this magic happens is actually the [OpenAI Codex](https://openai.com/blog/openai-codex/) system. [GitHub Copilot](https://github.com/features/copilot) also uses OpenAI Codex under the hood. One of the largest moves in the pure AI space is the new [$550m fund dedicated from Radical Venture, Toronto dedicated to investing in AI start ups](https://www.ft.com/content/118e353d-94b8-4025-a76c-bdf206fcfcb0). Radical Venture investors include [Fei-Fei Li](https://en.wikipedia.org/wiki/Fei-Fei_Li) (ImageNet) and [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton) (neural networks), expect to see more investments in the pure AI /generative learning / GPU / TPU space 

- **Of Poachers and Gamekeepers:** As a Data Platform practitioner accuracy, fairness, explicability of these AI models and the environmental considerations of training them (with trillion + parameters) are things I care deeply about. Yes, the new tech is amazing and will unlock humanity's potential in ways we haven't even begun to fathom. But unless the AI industry and regulators, watchdogs, auditors work together from onset we will repeat several adverse outcomes already seen in social media and crypto for example. One of the most thoughtful treatment of these topics that I have come across is the paper ["Challenges in Deploying Machine Learning: a Survey of Case Studies"](https://arxiv.org/abs/2011.09926) by three researchers at Cambridge, UK. It's sections on ethics, trust, security and how to think about tools and other holistic approaches to address these should be mandatory read as we embark upon the next stage of our journey  